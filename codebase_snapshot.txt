

# ── .pytest_cache/README.md

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


# ── .streamlit/README.md

# Streamlit Configuration Directory

This directory is used for Streamlit configuration files.

## Setting up API keys

To run this application locally, create a `secrets.toml` file in this directory with your API keys:

```toml
[edsl]
api_key = "your-edsl-api-key-here"

[edsl.openai]
api_key = "your-openai-api-key-here"

# Add other provider API keys as needed
```

See the main `secrets.toml.example` file at the root of the project for a complete example.

## Streamlit Community Cloud

For deployment to Streamlit Community Cloud, add these secrets in the app settings rather than in this file.

# ── .streamlit/secrets.toml

[edsl]
api_key = '2LzfxruQM4tiKLqpjKN6JxysmuNlWWGyBdYkMMD5Zzo'

# ── README.md

# love.dj

A dating simulator powered by AI that simulates conversations between two people on a first date.

## Features

- Create profiles for two people and simulate their first date conversation
- Add names for the participants to make the conversation more personalized
- Set the number of back-and-forth exchanges
- Choose which language model to use
- Set an optional location or theme for the date
- Get ratings from both participants on how the date went

## Installation

1. Clone this repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

## API Key Setup

The application requires API keys to use language models through EDSL:

### Local Development

1. Create a `.streamlit/secrets.toml` file in your project directory
2. Use the `secrets.toml.example` file as a template
3. Add your EDSL API key and any model provider API keys you want to use

Example `.streamlit/secrets.toml`:
```toml
[edsl]
api_key = "your-edsl-api-key-here"

[edsl.openai]
api_key = "your-openai-api-key-here"
```

### Streamlit Community Cloud Deployment

1. Go to your app's settings in Streamlit Community Cloud
2. Navigate to the "Secrets" section
3. Add your secrets in the same format as the `.streamlit/secrets.toml` file

## Usage

Run the application with:

```
streamlit run app.py
```

Then open your browser to the URL shown in the console (typically http://localhost:8501).

## Project Structure

- `app.py` - Main entry point for the Streamlit application
- `src/` - Source code directory
  - `models/` - Contains the agent and simulation logic
    - `agents.py` - Agent creation and interaction functions
    - `simulation.py` - Core date simulation logic
  - `ui/` - User interface components
    - `streamlit_app.py` - Streamlit UI setup and display functions
- `tests/` - Unit tests
  - `test_agents.py` - Tests for agent functionality
  - `test_simulation.py` - Tests for simulation logic

## Testing

Run tests with pytest:

```
pytest
```

## Credits

Built with [Streamlit](https://streamlit.io/) and [EDSL](https://github.com/expectedparrot/edsl).


# ── README_MODEL_DROPDOWN.md

# Model Dropdown Implementation

This document explains how the model dropdown in the Streamlit app works with EDSL's `Model.check_working_models()`.

## Overview

The model dropdown shows all available models from EDSL without any filtering or categorization. This is implemented in:

- `src/utils/models.py`: Contains the functions to get and format models
- `src/ui/streamlit_app.py`: Displays the models in a dropdown

## How It Works

1. When the Streamlit app starts, it calls `format_models_for_selectbox()` to get a list of all available models
2. This function calls `get_all_models()`, which directly uses EDSL's `Model.check_working_models()`
3. The models are extracted from EDSL's response, sorted alphabetically, and displayed in the dropdown
4. The dropdown is a simple Streamlit selectbox with no categorization or filtering

## Implementation Details

Our implementation handles both formats that EDSL's `Model.check_working_models()` might return:

1. A dictionary mapping provider names to lists of models (older EDSL versions)
   ```python
   {
     "openai": ["gpt-4o", "gpt-4-turbo", ...],
     "anthropic": ["claude-3-opus-20240229", ...],
     ...
   }
   ```

2. A list of lists where each inner list contains provider and model name (newer EDSL versions)
   ```python
   [
     ["openai", "gpt-4o", ...],
     ["anthropic", "claude-3-opus-20240229", ...],
     ...
   ]
   ```

The implementation extracts model names, removes duplicates, sorts them alphabetically, and ensures the default model (`gpt-4o`) is always included.

## Debugging

If you're seeing only `gpt-4o` in the dropdown:

1. Check if EDSL is properly installed: `pip install edsl`
2. Verify your EDSL API credentials and configuration
3. Run `python debug_models.py` to see what models EDSL is returning
4. Check `edsl_models.log` for detailed logs about model loading

You can also use our `simulate_edsl.py` script to see what would happen if EDSL returns a realistic set of models.

## Testing

The implementation is tested in `tests/test_models.py`, which verifies:

1. That EDSL's `Model.check_working_models()` returns models in the expected format
2. That our `get_all_models()` function correctly extracts model names
3. That our `format_models_for_selectbox()` function formats them for the Streamlit dropdown

The tests handle both EDSL formats and ensure the code works correctly in all cases.

# ── app.py

# app.py
"""
Tiny wrapper so `streamlit run app.py` still works.

All UI logic lives in *src/ui/layout.py*.
"""

from src.ui.layout import main

if __name__ == "__main__":
    main()


# ── check_models.py

#!/usr/bin/env python3
"""
Debug script for EDSL models.

This script retrieves all available models from EDSL using the same approach
as the main application, and prints them in a readable format for debugging.

Run this script to verify what models EDSL is actually returning.
"""
import json
import logging
from edsl import Model
from src.utils.models import get_all_models, format_models_for_selectbox

# Set up logging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler("edsl_models.log"),
                              logging.StreamHandler()])
logger = logging.getLogger(__name__)

def print_separator(title=None):
    """Print a separator line with optional title."""
    width = 80
    if title:
        logger.info(f"\n{'-' * 10} {title} {'-' * (width - 12 - len(title))}")
    else:
        logger.info(f"\n{'-' * width}")

logger.info("Starting EDSL model check")

# First step: Call EDSL directly
print_separator("Direct EDSL Model.check_working_models() Output")
try:
    # Get the available working models
    logger.info("Calling Model.check_working_models()")
    working_models = Model.check_working_models()
    
    # Print the output to understand its structure
    logger.info(f"Type of working_models: {type(working_models)}")
    
    # If it's a dictionary, print keys and values separately for better readability
    if isinstance(working_models, dict):
        logger.info(f"Keys in working_models: {list(working_models.keys())}")
        
        # For each provider, log how many models are available
        for key, values in working_models.items():
            if isinstance(values, list):
                logger.info(f"Provider '{key}' has {len(values)} models")
                if values:  # If there are models, log a sample
                    logger.info(f"  Sample: {values[:3]}")
            else:
                logger.info(f"Provider '{key}' has unexpected value type: {type(values)}")
    else:
        logger.info(f"working_models is not a dictionary but a {type(working_models)}")
        logger.info(f"Content: {working_models}")
    
    # Write the full output to a file for reference
    with open("edsl_models_output.json", "w") as f:
        json.dump(working_models, f, indent=2)
    logger.info("Wrote raw output to edsl_models_output.json")
        
except Exception as e:
    logger.error(f"Error getting models directly from EDSL: {e}", exc_info=True)

# Second step: Use our utility function
print_separator("Our get_all_models() Output")
try:
    all_models = get_all_models()
    model_count = len(all_models)
    logger.info(f"Total models: {model_count}")
    
    # Check if we have a substantial number of models
    if model_count < 100:
        logger.warning(f"WARNING: Only found {model_count} models. EDSL should return 100+ models!")
    else:
        logger.info(f"SUCCESS: Found {model_count} models (expecting 100+)")
        
    # Show a sample of the models
    logger.info(f"First 20 models: {', '.join(all_models[:20])}{'...' if len(all_models) > 20 else ''}")
    
    # Check for specific models we expect to see
    expected_models = [
        'gpt-4o', 
        'gpt-4-turbo', 
        'gpt-3.5-turbo', 
        'claude-3-opus-20240229',
        'claude-3-sonnet-20240229', 
        'gemini-1.5-pro'
    ]
    
    found_models = [model for model in expected_models if model in all_models]
    missing_models = [model for model in expected_models if model not in all_models]
    
    if found_models:
        logger.info(f"Found expected models: {', '.join(found_models)}")
    
    if missing_models:
        logger.warning(f"WARNING: Could not find some expected models: {', '.join(missing_models)}")
except Exception as e:
    logger.error(f"Error in get_all_models(): {e}", exc_info=True)

# Third step: Format for selectbox
print_separator("Our format_models_for_selectbox() Output")
try:
    formatted_models = format_models_for_selectbox()
    logger.info(f"Total models for dropdown: {len(formatted_models)}")
    logger.info(f"Default model: {formatted_models[0]}")
    logger.info(f"All models: {', '.join(formatted_models[:10])}{'...' if len(formatted_models) > 10 else ''}")
except Exception as e:
    logger.error(f"Error in format_models_for_selectbox(): {e}", exc_info=True)

# Save the processed data
print_separator("Saving Processed Output")
try:
    with open('edsl_processed_models.json', 'w') as f:
        json.dump({
            'all_models': all_models if 'all_models' in locals() else None,
            'formatted_models': formatted_models if 'formatted_models' in locals() else None
        }, f, indent=2)
    logger.info("Saved processed model data to edsl_processed_models.json")
except Exception as e:
    logger.error(f"Error saving processed output: {e}", exc_info=True)

print_separator()
logger.info("Debug complete.")

if 'all_models' in locals() and len(all_models) <= 1:
    logger.warning("CRITICAL ISSUE: Only showing gpt-4o in the dropdown!")
    logger.warning("The dropdown should contain 100+ models from various providers.")
    logger.warning("\nPossible causes:")
    logger.warning("1. EDSL's check_working_models() is not returning models (check edsl_models_output.json)")
    logger.warning("2. Models are not being properly flattened in get_all_models()")
    logger.warning("3. The formatted models are not being passed to the Streamlit selectbox")
else:
    logger.info("If Streamlit is showing all models correctly, you're good to go!")
    logger.info("If not, review the logs above for any warnings or errors")

logger.info("EDSL model check complete")

# ── codebase_snapshot.txt



# ── .pytest_cache/README.md

# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.


# ── .streamlit/README.md

# Streamlit Configuration Directory

This directory is used for Streamlit configuration files.

## Setting up API keys

To run this application locally, create a `secrets.toml` file in this directory with your API keys:

```toml
[edsl]
api_key = "your-edsl-api-key-here"

[edsl.openai]
api_key = "your-openai-api-key-here"

# Add other provider API keys as needed
```

See the main `secrets.toml.example` file at the root of the project for a complete example.

## Streamlit Community Cloud

For deployment to Streamlit Community Cloud, add these secrets in the app settings rather than in this file.

# ── .streamlit/secrets.toml

[edsl]
api_key = '2LzfxruQM4tiKLqpjKN6JxysmuNlWWGyBdYkMMD5Zzo'

# ── README.md

# love.dj

A dating simulator powered by AI that simulates conversations between two people on a first date.

## Features

- Create profiles for two people and simulate their first date conversation
- Add names for the participants to make the conversation more personalized
- Set the number of back-and-forth exchanges
- Choose which language model to use
- Set an optional location or theme for the date
- Get ratings from both participants on how the date went

## Installation

1. Clone this repository
2. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

## API Key Setup

The application requires API keys to use language models through EDSL:

### Local Development

1. Create a `.streamlit/secrets.toml` file in your project directory
2. Use the `secrets.toml.example` file as a template
3. Add your EDSL API key and any model provider API keys you want to use

Example `.streamlit/secrets.toml`:
```toml
[edsl]
api_key = "your-edsl-api-key-here"

[edsl.openai]
api_key = "your-openai-api-key-here"
```

### Streamlit Community Cloud Deployment

1. Go to your app's settings in Streamlit Community Cloud
2. Navigate to the "Secrets" section
3. Add your secrets in the same format as the `.streamlit/secrets.toml` file

## Usage

Run the application with:

```
streamlit run app.py
```

Then open your browser to the URL shown in the console (typically http://localhost:8501).

## Project Structure

- `app.py` - Main entry point for the Streamlit application
- `src/` - Source code directory
  - `models/` - Contains the agent and simulation logic
    - `agents.py` - Agent creation and interaction functions
    - `simulation.py` - Core date simulation logic
  - `ui/` - User interface components
    - `streamlit_app.py` - Streamlit UI setup and display functions
- `tests/` - Unit tests
  - `test_agents.py` - Tests for agent functionality
  - `test_simulation.py` - Tests for simulation logic

## Testing

Run tests with pytest:

```
pytest
```

## Credits

Built with [Streamlit](https://streamlit.io/) and [EDSL](https://github.com/expectedparrot/edsl).


# ── README_MODEL_DROPDOWN.md

# Model Dropdown Implementation

This document explains how the model dropdown in the Streamlit app works with EDSL's `Model.check_working_models()`.

## Overview

The model dropdown shows all available models from EDSL without any filtering or categorization. This is implemented in:

- `src/utils/models.py`: Contains the functions to get and format models
- `src/ui/streamlit_app.py`: Displays the models in a dropdown

## How It Works

1. When the Streamlit app starts, it calls `format_models_for_selectbox()` to get a list of all available models
2. This function calls `get_all_models()`, which directly uses EDSL's `Model.check_working_models()`
3. The models are extracted from EDSL's response, sorted alphabetically, and displayed in the dropdown
4. The dropdown is a simple Streamlit selectbox with no categorization or filtering

## Implementation Details

Our implementation handles both formats that EDSL's `Model.check_working_models()` might return:

1. A dictionary mapping provider names to lists of models (older EDSL versions)
   ```python
   {
     "openai": ["gpt-4o", "gpt-4-turbo", ...],
     "anthropic": ["claude-3-opus-20240229", ...],
     ...
   }
   ```

2. A list of lists where each inner list contains provider and model name (newer EDSL versions)
   ```python
   [
     ["openai", "gpt-4o", ...],
     ["anthropic", "claude-3-opus-20240229", ...],
     ...
   ]
   ```

The implementation extracts model names, removes duplicates, sorts them alphabetically, and ensures the default model (`gpt-4o`) is always included.

## Debugging

If you're seeing only `gpt-4o` in the dropdown:

1. Check if EDSL is properly installed: `pip install edsl`
2. Verify your EDSL API credentials and configuration
3. Run `python debug_models.py` to see what models EDSL is returning
4. Check `edsl_models.log` for detailed logs about model loading

You can also use our `simulate_edsl.py` script to see what would happen if EDSL returns a realistic set of models.

## Testing

The implementation is tested in `tests/test_models.py`, which verifies:

1. That EDSL's `Model.check_working_models()` returns models in the expected format
2. That our `get_all_models()` function correctly extracts model names
3. That our `format_models_for_selectbox()` function formats them for the Streamlit dropdown

The tests handle both EDSL formats and ensure the code works correctly in all cases.

# ── app.py

# app.py
"""
Tiny wrapper so `streamlit run app.py` still works.

All UI logic lives in *src/ui/layout.py*.
"""

from src.ui.layout import main

if __name__ == "__main__":
    main()


# ── check_models.py

#!/usr/bin/env python3
"""
Debug script for EDSL models.

This script retrieves all available models from EDSL using the same approach
as the main application, and prints them in a readable format for debugging.

Run this script to verify what models EDSL is actually returning.
"""
import json
import logging
from edsl import Model
from src.utils.models import get_all_models, format_models_for_selectbox

# Set up logging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.FileHandler("edsl_models.log"),
                              logging.StreamHandler()])
logger = logging.getLogger(__name__)

def print_separator(title=None):
    """Print a separator line with optional title."""
    width = 80
    if title:
        logger.info(f"\n{'-' * 10} {title} {'-' * (width - 12 - len(title))}")
    else:
        logger.info(f"\n{'-' * width}")

logger.info("Starting EDSL model check")

# First step: Call EDSL directly
print_separator("Direct EDSL Model.check_working_models() Output")
try:
    # Get the available working models
    logger.info("Calling Model.check_working_models()")
    working_models = Model.check_working_models()
    
    # Print the output to understand its structure
    logger.info(f"Type of working_models: {type(working_models)}")
    
    # If it's a dictionary, print keys and values separately for better readability
    if isinstance(working_models, dict):
        logger.info(f"Keys in working_models: {list(working_models.keys())}")
        
        # For each provider, log how many models are available
        for key, values in working_models.items():
            if isinstance(values, list):
                logger.info(f"Provider '{key}' has {len(values)} models")
                if values:  # If there are models, log a sample
                    logger.info(f"  Sample: {values[:3]}")
            else:
                logger.info(f"Provider '{key}' has unexpected value type: {type(values)}")
    else:
        logger.info(f"working_models is not a dictionary but a {type(working_models)}")
        logger.info(f"Content: {working_models}")
    
    # Write the full output to a file for reference
    with open("edsl_models_output.json", "w") as f:
        json.dump(working_models, f, indent=2)
    logger.info("Wrote raw output to edsl_models_output.json")
        
except Exception as e:
    logger.error(f"Error getting models directly from EDSL: {e}", exc_info=True)

# Second step: Use our utility function
print_separator("Our get_all_models() Output")
try:
    all_models = get_all_models()
    model_count = len(all_models)
    logger.info(f"Total models: {model_count}")
    
    # Check if we have a substantial number of models
    if model_count < 100:
        logger.warning(f"WARNING: Only found {model_count} models. EDSL should return 100+ models!")
    else:
        logger.info(f"SUCCESS: Found {model_count} models (expecting 100+)")
        
    # Show a sample of the models
    logger.info(f"First 20 models: {', '.join(all_models[:20])}{'...' if len(all_models) > 20 else ''}")
    
    # Check for specific models we expect to see
    expected_models = [
        'gpt-4o', 
        'gpt-4-turbo', 
        'gpt-3.5-turbo', 
        'claude-3-opus-20240229',
        'claude-3-sonnet-20240229', 
        'gemini-1.5-pro'
    ]
    
    found_models = [model for model in expected_models if model in all_models]
    missing_models = [model for model in expected_models if model not in all_models]
    
    if found_models:
        logger.info(f"Found expected models: {', '.join(found_models)}")
    
    if missing_models:
        logger.warning(f"WARNING: Could not find some expected models: {', '.join(missing_models)}")
except Exception as e:
    logger.error(f"Error in get_all_models(): {e}", exc_info=True)

# Third step: Format for selectbox
print_separator("Our format_models_for_selectbox() Output")
try:
    formatted_models = format_models_for_selectbox()
    logger.info(f"Total models for dropdown: {len(formatted_models)}")
    logger.info(f"Default model: {formatted_models[0]}")
    logger.info(f"All models: {', '.join(formatted_models[:10])}{'...' if len(formatted_models) > 10 else ''}")
except Exception as e:
    logger.error(f"Error in format_models_for_selectbox(): {e}", exc_info=True)

# Save the processed data
print_separator("Saving Processed Output")
try:
    with open('edsl_processed_models.json', 'w') as f:
        json.dump({
            'all_models': all_models if 'all_models' in locals() else None,
            'formatted_models': formatted_models if 'formatted_models' in locals() else None
        }, f, indent=2)
    logger.info("Saved processed model data to edsl_processed_models.json")
except Exception as e:
    logger.error(f"Error saving processed output: {e}", exc_info=True)

print_separator()
logger.info("Debug complete.")

if 'all_models' in locals() and len(all_models) <= 1:
    logger.warning("CRITICAL ISSUE: Only showing gpt-4o in the dropdown!")
    logger.warning("The dropdown should contain 100+ models from various providers.")
    logger.warning("\nPossible causes:")
    logger.warning("1. EDSL's check_working_models() is not returning models (check edsl_models_output.json)")
    logger.warning("2. Models are not being properly flattened in get_all_models()")
    logger.warning("3. The formatted models are not being passed to the Streamlit selectbox")
else:
    logger.info("If Streamlit is showing all models correctly, you're good to go!")
    logger.info("If not, review the logs above for any warnings or errors")

logger.info("EDSL model check complete")

# ── debug_models.py

#!/usr/bin/env python3
"""
Simple script to debug model dropdown issues.

This script tests each step of the model loading process and prints
detailed information to help diagnose problems.

Run this script in your environment to check if EDSL models are loading correctly.
"""
import sys

print("\n======== EDSL Model Debug ========")
print(f"Python version: {sys.version}")

# Step 1: Import EDSL
print("\n1. Importing EDSL...")
try:
    from edsl import Model
    print("✅ EDSL imported successfully")
except ImportError as e:
    print(f"❌ Failed to import EDSL: {e}")
    print("Make sure EDSL is installed: pip install edsl")
    sys.exit(1)

# Step 2: Check EDSL Model.check_working_models()
print("\n2. Calling Model.check_working_models()...")
try:
    edsl_result = Model.check_working_models()
    print(f"✅ Got response from EDSL: {type(edsl_result)}")
    
    # Parse model data based on response format
    all_model_names = []
    
    if isinstance(edsl_result, list):
        # Newer EDSL returns a list of model info: [[provider, model_name, ...], ...]
        print(f"EDSL returned a list with {len(edsl_result)} model entries")
        
        # Extract model names and count by provider
        provider_counts = {}
        for model_info in edsl_result:
            if isinstance(model_info, list) and len(model_info) >= 2:
                provider = model_info[0]
                model_name = model_info[1]
                
                # Count models per provider
                provider_counts[provider] = provider_counts.get(provider, 0) + 1
                
                # Add to our list of model names
                all_model_names.append(model_name)
        
        # Print provider statistics
        print(f"Found models from {len(provider_counts)} providers:")
        for provider, count in sorted(provider_counts.items()):
            print(f"  • {provider}: {count} models")
    
    elif isinstance(edsl_result, dict):
        # Older EDSL returns a dictionary: {provider: [model_names], ...}
        print(f"Found {len(edsl_result)} providers")
        
        # Count models per provider
        for provider, models in edsl_result.items():
            model_count = len(models) if isinstance(models, list) else 0
            print(f"  • {provider}: {model_count} models")
            if isinstance(models, list):
                all_model_names.extend(models)
    
    else:
        print(f"❌ Unexpected return type: {type(edsl_result)}")
        print(f"Value (sample): {str(edsl_result)[:200]}...")
        print("The rest of the analysis may not work correctly.")
    
    # Count unique models
    unique_models = set(all_model_names)
    unique_count = len(unique_models)
    print(f"Total unique models: {unique_count}")
    
    if unique_count < 10:
        print(f"⚠️ WARNING: Only found {unique_count} models - expecting 100+")
        
    # Check for common models
    common_models = ['gpt-4o', 'gpt-4-turbo', 'claude-3-opus-20240229', 'gemini-1.5-pro']
    found_models = [model for model in common_models if model in unique_models]
    missing_models = [model for model in common_models if model not in unique_models]
    
    if found_models:
        print(f"Found expected models: {', '.join(found_models)}")
    
    if missing_models:
        print(f"⚠️ Missing expected models: {', '.join(missing_models)}")
    
    # Save raw data to file for inspection
    try:
        import json
        with open("edsl_models_raw.json", "w") as f:
            json.dump(edsl_result, f, indent=2)
        print("✅ Saved raw model data to edsl_models_raw.json")
    except:
        with open("edsl_models_raw.txt", "w") as f:
            f.write(str(edsl_result))
        print("✅ Saved raw model data to edsl_models_raw.txt")
        
except Exception as e:
    print(f"❌ Error calling Model.check_working_models(): {e}")
    print("Check EDSL configuration and API credentials")
    sys.exit(1)

# Step 3: Check our utility functions
print("\n3. Testing our model utility functions...")
try:
    from src.utils.models import get_all_models, format_models_for_selectbox
    print("✅ Imported model utilities")
    
    # Test get_all_models
    print("\nTesting get_all_models()...")
    all_models = get_all_models()
    print(f"✅ Got {len(all_models)} models")
    print(f"Sample: {', '.join(all_models[:5])}...")
    
    # Test format_models_for_selectbox
    print("\nTesting format_models_for_selectbox()...")
    formatted_models = format_models_for_selectbox()
    print(f"✅ Got {len(formatted_models)} formatted models")
    print(f"First model: {formatted_models[0]}")
    
except ImportError as e:
    print(f"❌ Failed to import model utilities: {e}")
    print("Make sure you're running this script from the project root")
except Exception as e:
    print(f"❌ Error in model utilities: {e}")

print("\n======== Debug Complete ========")
print("If you're seeing models in steps 2 and 3, but not in the Streamlit app,")
print("check your Streamlit app's model dropdown implementation.")
print("If not seeing models in step 2, check your EDSL installation and API credentials.")

# ── dump_codebase.py

# dump_codebase.py
"""
Collect (small) codebases into one text file for easy sharing / review.

• Run from the repo root:   python dump_codebase.py
• It will create           ./codebase_snapshot.txt
• The script skips virtual-envs, __pycache__, git metadata, and anything
  over ~200 KB just in case a huge asset sneaked in.
"""

from pathlib import Path
import os

ROOT = Path(__file__).resolve().parent  # repo root
OUTPUT = ROOT / "codebase_snapshot.txt"
MAX_KB = 200  # skip giant blobs
KEEP_EXT = {".py", ".md", ".toml", ".txt"}  # tweak if needed
SKIP_DIRS = {".git", ".venv", "venv", "node_modules", "__pycache__"}


def wanted(path: Path) -> bool:
    if any(part in SKIP_DIRS for part in path.parts):
        return False
    if path.is_dir():
        return False
    if path.suffix.lower() not in KEEP_EXT:
        return False
    if path.stat().st_size > MAX_KB * 1024:
        return False
    return True


with OUTPUT.open("w", encoding="utf-8") as fout:
    for file in sorted(ROOT.rglob("*")):
        if not wanted(file):
            continue

        rel = file.relative_to(ROOT)
        fout.write(f"\n\n# ── {rel}\n\n")
        try:
            fout.write(file.read_text(encoding="utf-8", errors="replace"))
        except Exception as exc:
            fout.write(f"(Could not read file: {exc})")

print(
    f"\n✅  Wrote snapshot → {OUTPUT.relative_to(ROOT)}  "
    f"({OUTPUT.stat().st_size/1024:.1f} KB)"
)


# ── requirements.txt

streamlit>=1.26.0
edsl @ git+https://github.com/expectedparrot/edsl@main
pytest>=7.4.0

# ── simulate_edsl.py

#!/usr/bin/env python3
"""
Test script to simulate EDSL Model.check_working_models().

This script creates a mock of the EDSL Model class that returns
a realistic set of models similar to what the real EDSL would return.
"""
import sys
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# Create a mock for the EDSL Model class
class MockModel:
    @staticmethod
    def check_working_models():
        """Simulate what the real EDSL would return."""
        logger.info("Mock EDSL returning simulated models...")
        
        # This is what EDSL would typically return - a dictionary of providers to lists of models
        return {
            "openai": [
                "gpt-4o",
                "gpt-4-turbo",
                "gpt-3.5-turbo",
                "gpt-4-vision-preview",
                "gpt-4-0125-preview",
                "gpt-4-1106-preview",
                "gpt-4-0613",
                "gpt-4",
                "gpt-3.5-turbo-1106"
            ],
            "anthropic": [
                "claude-3-opus-20240229",
                "claude-3-sonnet-20240229",
                "claude-3-haiku-20240307",
                "claude-2.1",
                "claude-2.0",
                "claude-instant-1.2"
            ],
            "google": [
                "gemini-1.5-pro",
                "gemini-1.5-flash",
                "gemini-1.0-pro",
                "gemini-1.0-pro-vision"
            ],
            "mistral": [
                "mistral-large-latest",
                "mistral-medium-latest",
                "mistral-small-latest"
            ],
            "groq": [
                "llama-2-70b-chat",
                "llama-3-70b-8192",
                "llama-3-8b-8192",
                "mixtral-8x7b-32768",
                "gemma-7b-it"
            ]
        }

# Modify sys.modules to inject our mock
sys.modules['edsl'] = type('edsl', (), {'Model': MockModel})

# Now import our models utility to test it with the mock
try:
    from src.utils.models import get_all_models, format_models_for_selectbox
    
    # Get all the models as a flat list
    all_models = get_all_models()
    print(f"\nFound {len(all_models)} models from mock EDSL")
    
    # Get the models formatted for the selectbox
    formatted_models = format_models_for_selectbox()
    print(f"Formatted {len(formatted_models)} models for the selectbox")
    
    # Show the models
    print("\nFirst 10 models in the selectbox:")
    for i, model in enumerate(formatted_models[:10]):
        print(f"{i+1}. {model}")
    
    # Now simulate what the Streamlit app would do
    print("\nSimulating Streamlit selectbox:")
    
    # Find the default model index
    default_model_index = 0
    for i, model in enumerate(formatted_models):
        if model == "gpt-4o":
            default_model_index = i
            break
            
    print(f"Default model index: {default_model_index}")
    print(f"Default model: {formatted_models[default_model_index]}")
    
    # Check that we have a good number of models
    if len(formatted_models) < 10:
        print("\n⚠️ WARNING: Very few models found, the dropdown would be almost empty!")
    else:
        print(f"\n✅ PASS: Dropdown would show {len(formatted_models)} models")
        
except ImportError as e:
    print(f"Error importing model utils: {e}")
except Exception as e:
    print(f"Error in model utils: {e}")

# ── src/__init__.py

# src/__init__.py
# This file is intentionally left empty to make the directory a Python package


# ── src/models/__init__.py

# src/models/__init__.py
# This file is intentionally left empty to make the directory a Python package


# ── src/models/agents.py

# src/models/agents.py
from edsl import (
    Agent,
    Model,
    Scenario,
    QuestionFreeText,
    QuestionLinearScale,
)

# Default profiles if user doesn't provide any
DEFAULT_PROFILES = {
    "default_a": "28 year old product manager living in San Francisco. Enjoys jazz music, rock climbing on weekends, and trying new restaurants. Looking for someone who is adventurous and has a good sense of humor.",
    "default_b": "30 year old PhD student in literature. Avid reader, enjoys philosophical discussions, practicing yoga, and is a committed vegan. Values intellectual curiosity and authenticity in relationships.",
}

# Conversation guidelines
CONVERSATION_GUIDELINES = (
    "Speak casually, in first-person, as if you're meeting for the first time. "
    "Do **not** dump your full résumé; reveal details gradually and ask questions. "
    "Feel free to use humour or small-talk."
)


def create_agent(name, profile, default_profile):
    """Create an agent with the given name and profile."""
    return Agent(
        name=name,
        traits={
            "persona": profile or default_profile,
            "guidelines": CONVERSATION_GUIDELINES,
        },
    )


def get_opener(model_name, agent, service_name=None):
    """Get an opening message from an agent."""
    if service_name:
        model = Model(model_name, service_name=service_name)
        print(f"Model instance created for opener with service {service_name}: {model}")
    else:
        model = Model(model_name)
        print(f"Model instance created for opener: {model}")
    
    # Create a scenario with gender information if it exists
    if "gender" in agent.traits:
        scenario = Scenario({
            "persona": agent.traits["persona"],
            "gender": agent.traits["gender"],
        })
        
        return (
            QuestionFreeText(
                question_name="opener",
                question_text=(
                    "You are {{ persona }} (using {{ gender }} pronouns) on a first date. "
                    "Write a brief opening statement to introduce yourself and ask a question. "
                    "DO NOT include your name at the beginning of your response as it will be added automatically. "
                    "Just write your dialogue directly."
                ),
            )
            .by(model)
            .by(agent)
            .by(scenario)
            .run()
            .select("opener")
            .first()
        )
    else:
        # Fallback to original behavior if no gender information
        return (
            QuestionFreeText(
                question_name="opener",
                question_text=(
                    "You are on a first date. Write a brief opening statement to introduce yourself and ask a question. "
                    "DO NOT include your name at the beginning of your response as it will be added automatically. "
                    "Just write your dialogue directly."
                ),
            )
            .by(model)
            .by(agent)
            .run()
            .select("opener")
            .first()
        )


def get_response(model_name, agent_self, agent_other, turn, speaker, history_txt, service_name=None):
    """Get a response from an agent based on conversation history."""
    if service_name:
        model = Model(model_name, service_name=service_name)
        print(f"Model instance created for response with service {service_name}: {model}")
    else:
        model = Model(model_name)
        print(f"Model instance created for response: {model}")
    
    # Create a dictionary of scenario data with required fields
    scenario_data = {
        "chat": history_txt.strip(),
        "persona": agent_self.traits["persona"],
        "partner_persona": agent_other.traits["persona"],
    }
    
    # Add gender information if available
    if "gender" in agent_self.traits:
        scenario_data["gender"] = agent_self.traits["gender"]
    else:
        # Default gender if not specified
        scenario_data["gender"] = "he/him"
        
    if "gender" in agent_other.traits:
        scenario_data["partner_gender"] = agent_other.traits["gender"]
    
    # Create the scenario with the data
    scenario = Scenario(scenario_data)

    q = QuestionFreeText(
        question_name=f"turn_{turn}_{speaker}",
        question_text=(
            "You are {{ persona }} (using {{ gender }} pronouns) on a first date with {{ partner_persona }}. \n\n"
            "{{ chat }}\n\n"
            "Respond in character (≤ 120 words). DO NOT include your name at the beginning of your response as "
            "it will be added automatically. Just write your dialogue directly."
        ),
    )

    return (
        q.by(model)
        .by(agent_self)
        .by(scenario)
        .run()
        .select(f"turn_{turn}_{speaker}")
        .first()
    )


def get_rating(model_name, agent, history_txt, service_name=None):
    """Get a rating from an agent based on conversation history."""
    if service_name:
        model = Model(model_name, service_name=service_name)
        print(f"Model instance created for rating with service {service_name}: {model}")
    else:
        model = Model(model_name)
        print(f"Model instance created for rating: {model}")
    
    rating_question = QuestionLinearScale(
        question_name="rating",
        question_text=(
            "{{ history }}\n\n"
            "On a scale of 1–10, how would you rate this date? "
            "(1 = Terrible • 10 = Amazing)\n"
            "IMPORTANT: Respond with just the number from 1-10. No words or explanations."
        ),
        question_options=list(range(1, 11)),  # 1-10 inclusive
        option_labels={1: "Terrible", 10: "Amazing"},
    )

    # Create scenario data with history
    scenario_data = {"history": history_txt}
    
    # Add gender information if available
    if "gender" in agent.traits:
        scenario_data["gender"] = agent.traits["gender"]
    
    result = (
        rating_question.by(model)
        .by(agent)
        .by(Scenario(scenario_data))
        .run()
        .select("rating")
        .first()
    )
    
    # Handle case where result might be None
    if result is None:
        return 5  # Default middle rating if no response
    
    # Try to convert to int, with fallback
    try:
        return int(result)
    except (ValueError, TypeError):
        # Try to extract a number if result is a string with text
        if isinstance(result, str):
            import re
            numbers = re.findall(r'\d+', result)
            if numbers:
                return int(numbers[0])
        return 5  # Default to middle rating if conversion fails


# ── src/models/simulation.py

# src/models/simulation.py
"""
Conversation-simulation helpers for *love.dj*.

The file now has **two layers**:

1.  A *deterministic* `run_date()` used by the pytest suite
    (no external API cost – unchanged).
2.  A set of step-by-step helpers (`initialize_date`, `get_next_response`,
    etc.) that **call EDSL live** via `src.models.agents`, enabling the
    streaming UI.
"""

from __future__ import annotations

import random
from typing import List, Tuple, Optional


# ---------------------------------------------------------------------------#
#  Section 1 – deterministic implementation used by the tests                #
# ---------------------------------------------------------------------------#
def _fake_rating() -> float:
    return round(random.uniform(6.0, 10.0), 1)


def run_date(
    *,
    name_a: str,
    profile_a: str,
    gender_a: str,
    name_b: str,
    profile_b: str,
    gender_b: str,
    rounds: int = 3,
    theme: Optional[str] = None,
    model_name: str = "gpt-4o",
    service_name: Optional[str] = None,
) -> Tuple[List[Tuple[str, str]], float | None, float | None]:
    """
    **Deterministic** mini-sim used only by `tests/test_simulation.py`.
    It does *not* hit EDSL so the test-suite stays fast & free.
    """
    transcript: List[Tuple[str, str]] = []
    for n in range(rounds):
        transcript.append(("A", f"Utterance {n+1} from {name_a or 'A'}"))
        transcript.append(("B", f"Response  {n+1} from {name_b or 'B'}"))

    return transcript, _fake_rating(), _fake_rating()


# ---------------------------------------------------------------------------#
#  Section 2 – **live** helpers for the Streamlit UI                         #
# ---------------------------------------------------------------------------#
from .agents import (  # heavy imports kept separate so the tests above remain cheap
    create_agent,
    get_opener,
    get_response,
    get_rating,
    DEFAULT_PROFILES,
)

# Caches for the “legacy”/step-wise API
_cached_agents: Tuple | None = None
_cached_transcript: List[Tuple[str, str]] = []
_cached_index: int = 0
_cached_history_txt: str = ""


def initialize_date(
    profile_a: str,
    profile_b: str,
    name_a: str,
    name_b: str,
    model_name: str,
    theme: Optional[str],
    service_name: Optional[str],
    gender_a: str,
    gender_b: str,
    rounds: int = 3,
):
    """
    Build the two agents **with EDSL traits** and return them together with
    display-names.  Side-effect: fills the module-level caches so subsequent
    calls to `get_opening_message()` / `get_next_response()` have context.
    """
    global _cached_agents, _cached_transcript, _cached_index, _cached_history_txt

    display_a = name_a.strip() or "A"
    display_b = name_b.strip() or "B"

    # create EDSL Agents
    agent_a = create_agent(display_a, profile_a, DEFAULT_PROFILES["default_a"])
    agent_a.traits["gender"] = gender_a
    agent_b = create_agent(display_b, profile_b, DEFAULT_PROFILES["default_b"])
    agent_b.traits["gender"] = gender_b

    _cached_agents = (agent_a, agent_b, display_a, display_b, model_name, service_name)
    _cached_transcript = []
    _cached_index = 0
    _cached_history_txt = ""

    return agent_a, agent_b, display_a, display_b


def get_opening_message(
    agent_a,
    display_a: str,
    model_name: str,
    service_name: Optional[str],
):
    """Ask **Agent A** for the opening line."""
    opener = get_opener(model_name, agent_a, service_name=service_name)
    entry = (display_a, opener)
    _update_history(entry)
    return entry, _cached_history_txt


def get_next_response(
    agent_self,
    agent_other,
    display_self: str,
    turn: int,
    speaker: str,
    history_txt: str,
    model_name: str,
    service_name: Optional[str],
):
    """Ask the current speaker for their reply."""
    response = get_response(
        model_name,
        agent_self,
        agent_other,
        turn,
        speaker,
        history_txt,
        service_name=service_name,
    )
    entry = (display_self, response)
    _update_history(entry)
    return entry, _cached_history_txt


def get_date_ratings(
    agent_a,
    agent_b,
    history_txt: str,
    model_name: str,
    service_name: Optional[str],
):
    """Fetch linear-scale scores (1–10) from both agents."""
    score_a = get_rating(model_name, agent_a, history_txt, service_name=service_name)
    score_b = get_rating(model_name, agent_b, history_txt, service_name=service_name)
    return score_a, score_b


# ───────── internal helper ──────────────────────────────────────────────────
def _update_history(entry: Tuple[str, str]) -> None:
    """Append the new message to the module-level history string."""
    global _cached_history_txt
    speaker, msg = entry
    _cached_history_txt += f"\n{speaker}: {msg}"
    _cached_transcript.append(entry)


# ── src/ui/__init__.py

# src/ui/__init__.py
"""
Public UI façade – lets the rest of the project keep its old imports.

All helpers are re-exported so code that does
    from src.ui.streamlit_app import setup_ui, ...
continues to work.
"""

from .layout import setup_ui, main  # noqa: F401
from .transcript import (  # noqa: F401
    create_real_time_transcript_container,
    update_transcript,
)
from .results import display_results  # noqa: F401

__all__ = [
    "setup_ui",
    "main",
    "create_real_time_transcript_container",
    "update_transcript",
    "display_results",
]


# ── src/ui/layout.py

# src/ui/layout.py
"""
Streamlit front-end that **streams** each EDSL reply.

It uses the live helpers from `src.models.simulation` and the small
avatar/emoji transcript UI from `src.ui.transcript`.
"""
from __future__ import annotations

import time
import streamlit as st
from typing import List, Tuple

from src.utils.models import format_models_for_selectbox, get_service_map
from src.ui.transcript import (
    create_real_time_transcript_container,
    update_transcript,
)
from src.ui.results import display_results
from src.models.simulation import (
    initialize_date,
    get_opening_message,
    get_next_response,
    get_date_ratings,
)


# ────────────────────────────────────────────────────────────────────────────
def _form() -> dict:
    """Render input widgets and return the selections."""
    st.set_page_config(page_title="🎧 love.dj", page_icon="🎧")
    st.title("🎧 love.dj")

    # profiles ---------------------------------------------------------------
    c1, c2 = st.columns(2)
    with c1:
        name_a = st.text_input("Person A’s name")
        gender_a = st.selectbox(
            "Person A’s pronouns", ["he/him", "she/her", "they/them"]
        )
        profile_a = st.text_area("Profile A", height=140)

    with c2:
        name_b = st.text_input("Person B’s name")
        gender_b = st.selectbox(
            "Person B’s pronouns", ["he/him", "she/her", "they/them"], index=1
        )
        profile_b = st.text_area("Profile B", height=140)

    # date settings ----------------------------------------------------------
    st.subheader("Date settings")
    c3, c4 = st.columns(2)
    with c3:
        rounds = st.slider("Back-and-forth rounds", 1, 6, 3)

        opts = format_models_for_selectbox()
        default_ix = opts.index("gpt-4o [openai]") if "gpt-4o [openai]" in opts else 0
        chosen = st.selectbox("Language model", opts, index=default_ix)
        model_name = chosen.rsplit(" ", 1)[0]  # strip " [provider]"

    with c4:
        theme = st.text_input("Location / theme (optional)")

    go = st.button("🚀 Spin the decks")

    return dict(
        name_a=name_a,
        profile_a=profile_a,
        gender_a=gender_a,
        name_b=name_b,
        profile_b=profile_b,
        gender_b=gender_b,
        rounds=rounds,
        theme=theme,
        model_name=model_name,
        go=go,
    )


# ────────────────────────────────────────────────────────────────────────────
def main() -> None:
    ui = _form()
    if not ui["go"]:
        return

    # provider lookup --------------------------------------------------------
    provider_map = get_service_map()
    service = provider_map.get(ui["model_name"])
    if service is None:
        st.error("Couldn’t find which service hosts that model. Pick another.")
        return

    st.info(f"Using **{ui['model_name']}** via *{service}* service …")

    # transcript container ---------------------------------------------------
    container, placeholders, messages = create_real_time_transcript_container()

    # initialise agents & opener --------------------------------------------
    agent_a, agent_b, disp_a, disp_b = initialize_date(
        ui["profile_a"],
        ui["profile_b"],
        ui["name_a"],
        ui["name_b"],
        ui["model_name"],
        ui["theme"],
        service,
        ui["gender_a"],
        ui["gender_b"],
        ui["rounds"],
    )

    opener_entry, history = get_opening_message(
        agent_a, disp_a, ui["model_name"], service
    )
    update_transcript(
        container,
        placeholders,
        messages,
        "A",
        opener_entry[1],
        ui["gender_a"],
        ui["gender_b"],
    )

    # dialogue rounds --------------------------------------------------------
    for turn in range(ui["rounds"]):
        time.sleep(0.4)

        # B’s reply
        b_entry, history = get_next_response(
            agent_b,
            agent_a,
            disp_b,
            turn,
            "B",
            history,
            ui["model_name"],
            service,
        )
        update_transcript(
            container,
            placeholders,
            messages,
            "B",
            b_entry[1],
            ui["gender_a"],
            ui["gender_b"],
        )

        time.sleep(0.4)

        # A’s reply
        a_entry, history = get_next_response(
            agent_a,
            agent_b,
            disp_a,
            turn,
            "A",
            history,
            ui["model_name"],
            service,
        )
        update_transcript(
            container,
            placeholders,
            messages,
            "A",
            a_entry[1],
            ui["gender_a"],
            ui["gender_b"],
        )

    # ratings ----------------------------------------------------------------
    score_a, score_b = get_date_ratings(
        agent_a, agent_b, history, ui["model_name"], service
    )

    display_results(
        transcript=[],  # we already printed lines live
        score_a=score_a,
        score_b=score_b,
        name_a=ui["name_a"],
        name_b=ui["name_b"],
        model_name=ui["model_name"],
    )


# convenience:  python -m src.ui.layout  -> launches Streamlit
if __name__ == "__main__":
    import sys, streamlit.web.cli as stcli

    sys.argv = ["streamlit", "run", __file__]
    sys.exit(stcli.main())

# ------------------------------------------------------------------ #
#  Back-compat: keep the old public name “setup_ui”                  #
# ------------------------------------------------------------------ #
setup_ui = _form  # ← add this line


# ── src/ui/results.py

# src/ui/results.py
import streamlit as st
from typing import List, Tuple


def display_results(
    transcript: List[Tuple[str, str]],
    score_a: float | None,
    score_b: float | None,
    name_a: str,
    name_b: str,
    model_name: str,
):
    """Show transcript and (optionally) the scores."""
    st.success(f"Simulated with **{model_name}**")

    st.subheader("Conversation")
    for who, line in transcript:
        st.markdown(f"**{who}:** {line}")

    if score_a is not None and score_b is not None:
        st.subheader("⭐ Ratings")
        c1, c2, c3 = st.columns(3)
        c1.metric(f"{name_a or 'A'}", f"{score_a}/10")
        c2.metric(f"{name_b or 'B'}", f"{score_b}/10")
        c3.metric("Average", f"{(score_a + score_b)/2:.1f}/10")


# ── src/ui/streamlit_app.py

# src/ui/streamlit_app.py
import streamlit as st
import os
from src.models.simulation import run_date
from src.utils.models import format_models_for_selectbox


def setup_ui():
    """Set up the Streamlit UI."""
    st.set_page_config(page_title="🎧 love.dj", page_icon="🎧")
    st.title("🎧 love.dj")

    col1, col2 = st.columns(2)
    with col1:
        name_a = st.text_input("Person A's name", value="")

        # Gender selection for person A
        gender_a = st.selectbox(
            "Person A's gender",
            options=["he/him", "she/her", "they/them"],
            index=0,
            help="This affects pronouns and emoji representation",
        )

        profile_a = st.text_area(
            "Profile A (who is *this* person?)",
            height=150,
            placeholder="e.g. 28 y/o product-manager, loves jazz & climbing…",
        )
    with col2:
        name_b = st.text_input("Person B's name", value="")

        # Gender selection for person B
        gender_b = st.selectbox(
            "Person B's gender",
            options=["he/him", "she/her", "they/them"],
            index=1,  # Default to she/her for person B
            help="This affects pronouns and emoji representation",
        )

        profile_b = st.text_area(
            "Profile B (and their date?)",
            height=150,
            placeholder="e.g. 30 y/o PhD student, avid reader, vegan…",
        )

    # Additional options
    st.subheader("Date Settings")
    col3, col4 = st.columns(2)
    with col3:
        rounds = st.slider("How many back-and-forths?", 1, 6, value=3)

        # Get models for the dropdown directly from EDSL
        all_models = format_models_for_selectbox()

        # Find the index of the default model (gpt-4o)
        default_model_index = 0
        for i, model in enumerate(all_models):
            if model == "gpt-4o":
                default_model_index = i
                break

        # Add help text for the model selector
        model_help = """
        Select a language model to use for the date simulation.
        
        These models are retrieved directly from EDSL's available models.
        """

        # Create a simple selectbox for model selection
        model_name = st.selectbox(
            "Language Model",
            options=all_models,
            index=default_model_index,
            help=model_help,
        )

        # Service name is always auto-detected
        service_name = "None (auto)"
    with col4:
        theme = st.text_input(
            "Date location/theme (optional)",
            value="",
            placeholder="e.g. a coffee shop, hiking trail, fancy restaurant...",
        )

    go = st.button("🚀 Spin the decks")

    # Add footer with EDSL info
    st.markdown("---")
    st.markdown(
        """
        <div style="text-align: center; font-size: 0.8em; color: #888;">
        Powered by <a href="https://www.expectedparrot.com/" target="_blank">EDSL</a> • 
        <a href="https://www.expectedparrot.com/models" target="_blank">Supported Models</a> •
        <a href="https://github.com/expectedparrot/edsl" target="_blank">GitHub</a>
        </div>
        """,
        unsafe_allow_html=True,
    )

    return {
        "name_a": name_a,
        "profile_a": profile_a,
        "gender_a": gender_a,
        "name_b": name_b,
        "profile_b": profile_b,
        "gender_b": gender_b,
        "rounds": rounds,
        "model_name": model_name,
        "service_name": None,  # Always use auto-detection
        "theme": theme,
        "go": go,
    }


def create_real_time_transcript_container():
    """Create a container for real-time transcript display.

    Returns:
        tuple: (transcript_container, message_placeholders, transcript_messages)
              where message_placeholders is for real-time updates and
              transcript_messages keeps track of all messages
    """
    st.subheader("💬 Transcript")

    # Add some CSS to style the transcript container
    st.markdown(
        """
    <style>
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .chat-message.user-a {
        background-color: rgba(240, 242, 246, 0.5);
        border-left: 5px solid #9AD1F5;
    }
    .chat-message.user-b {
        background-color: rgba(240, 242, 246, 0.5);
        border-left: 5px solid #F5C3A9;
    }
    .chat-message .avatar {
        font-size: 1.25rem;
        width: 2.5rem;
        height: 1.5rem;
        text-align: center;
        float: left;
        margin-right: 1rem;
    }
    .chat-message .message {
        flex-grow: 1;
    }
    .chat-message .name {
        font-weight: bold;
        font-size: 0.9rem;
        color: #424242;
    }
    .typing-indicator {
        display: inline-block;
        margin-left: 5px;
    }
    .typing-indicator span {
        display: inline-block;
        background-color: #808080;
        width: 5px;
        height: 5px;
        border-radius: 50%;
        margin: 0 1px;
        animation: typing 1.5s infinite ease-in-out;
    }
    .typing-indicator span:nth-child(2) {
        animation-delay: 0.2s;
    }
    .typing-indicator span:nth-child(3) {
        animation-delay: 0.4s;
    }
    @keyframes typing {
        0%, 60%, 100% {
            transform: translateY(0);
        }
        30% {
            transform: translateY(-5px);
        }
    }
    </style>
    """,
        unsafe_allow_html=True,
    )

    # Create a container with a bordered style
    transcript_container = st.container()
    # List to store message placeholders for real-time updates
    message_placeholders = []
    # List to track all transcript messages
    transcript_messages = []

    return transcript_container, message_placeholders, transcript_messages


def update_transcript(
    transcript_container,
    message_placeholders,
    transcript_messages,
    speaker,
    message,
    gender_a="he/him",
    gender_b="she/her",
):
    """Add a new message to the transcript in real-time.

    Args:
        transcript_container: The container to display messages in
        message_placeholders: List of message placeholders
        transcript_messages: List of all transcript messages
        speaker: The name of the speaker
        message: The message content
        gender_a: Gender/pronouns of person A
        gender_b: Gender/pronouns of person B
    """
    with transcript_container:
        # Create a new placeholder for this message if needed
        if len(message_placeholders) <= len(transcript_messages):
            placeholder = st.empty()
            message_placeholders.append(placeholder)
        else:
            placeholder = message_placeholders[len(transcript_messages)]

        # Determine which user class to use (for styling)
        if not transcript_messages:
            # This is the first message
            user_class = "user-a"
        else:
            user_class = (
                "user-a"
                if speaker.lower() == "a"
                or speaker.lower() == transcript_messages[0][0].lower()
                else "user-b"
            )

        # Get appropriate emoji based on gender
        gender = gender_a if user_class == "user-a" else gender_b

        if gender == "he/him":
            emoji = "👨"
        elif gender == "she/her":
            emoji = "👩"
        else:  # they/them
            emoji = "🧑"

        # Format message with HTML for styling
        html_message = f"""
        <div class="chat-message {user_class}">
            <div class="avatar">
                {emoji}
            </div>
            <div class="message">
                <div class="name">{speaker}</div>
                {message}
            </div>
        </div>
        """

        # Display the message with custom HTML
        placeholder.markdown(html_message, unsafe_allow_html=True)

        # Add to our transcript list
        transcript_messages.append((speaker, message))


def display_results(
    transcript,
    score_a,
    score_b,
    name_a,
    name_b,
    model_name="",
    transcript_container=None,
    message_placeholders=None,
    transcript_messages=None,
):
    """Display the results of the date simulation."""
    if model_name:
        st.success(f"Date simulated with model: {model_name}")

    # Display ratings if we have them
    if score_a is not None and score_b is not None:
        st.subheader("⭐ Ratings")

        # Use the provided names or default to A/B
        display_a = name_a if name_a else "A"
        display_b = name_b if name_b else "B"

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(f"{display_a}'s score", f"{score_a}/10")
        with col2:
            st.metric(f"{display_b}'s score", f"{score_b}/10")
        with col3:
            st.metric("Average", f"{(score_a+score_b)/2:.1f}/10")

        st.caption(
            "Powered by EDSL – agents, scenarios & questions handle the heavy lifting of multi-agent dialogue"
        )


# ── src/ui/transcript.py

# src/ui/transcript.py
import streamlit as st
from typing import List, Tuple


def create_real_time_transcript_container():
    """Return (container, placeholders, messages)."""
    st.subheader("💬 Transcript")
    container = st.container()
    placeholders: List[st.delta_generator.DeltaGenerator] = []
    messages: List[Tuple[str, str]] = []
    return container, placeholders, messages


def update_transcript(
    container,
    placeholders,
    messages,
    speaker: str,
    text: str,
    gender_a="he/him",
    gender_b="she/her",
):
    """Append a new line of dialogue to the transcript."""
    # pick or make placeholder
    if len(placeholders) <= len(messages):
        placeholders.append(container.empty())
    ph = placeholders[len(messages)]

    # quick emoji from pronouns
    def emoji(gen: str) -> str:
        return {"he/him": "👨", "she/her": "👩"}.get(gen, "🧑")

    icon = emoji(gender_a if speaker == "A" else gender_b)

    ph.markdown(f"**{icon} {speaker}:** {text}")
    messages.append((speaker, text))


# ── src/utils/__init__.py

# src/utils/__init__.py
# This file is intentionally left empty to make the directory a Python package

# ── src/utils/models.py

"""
Helpers around EDSL Model plus a tiny registry of *service ⇄ model* pairs.

• get_all_models()               → flat, sorted list of model IDs
• get_service_map()              → {model_id: service_name}
• format_models_for_selectbox()  → human-friendly strings for a Streamlit box
"""

from __future__ import annotations

import logging
import os
from collections.abc import Sequence
from typing import Dict, List, Set, Tuple

# --------------------------------------------------------------------------- #
#  Logging                                                                    #
# --------------------------------------------------------------------------- #
FILE = os.path.abspath(__file__)
logging.basicConfig(
    level=logging.INFO,
    handlers=[logging.StreamHandler(), logging.FileHandler("edsl_models.log", "a")],
    format="%(asctime)s  %(levelname)s  %(name)s  %(message)s",
)
log = logging.getLogger("edsl_models")
log.info("Loaded helper module from %s", FILE)

# --------------------------------------------------------------------------- #
#  EDSL import                                                                #
# --------------------------------------------------------------------------- #
try:
    from edsl import Model  # type: ignore
except Exception as exc:  # pragma: no cover
    log.error("Could not import EDSL: %s", exc)
    Model = None  # type: ignore[assignment]


# --------------------------------------------------------------------------- #
#  Internal normalisation helpers                                             #
# --------------------------------------------------------------------------- #
def _normalise(raw) -> Tuple[List[str], Dict[str, str]]:
    """
    Convert whatever `Model.check_working_models()` returns to

        (sorted_unique_models, {model → service})
    """
    names: Set[str] = set()
    svc_map: Dict[str, str] = {}

    # PrettyList inherits from list/Sequence, so this covers legacy & new
    if isinstance(raw, Sequence):
        for row in raw:
            if isinstance(row, Sequence) and len(row) >= 2:
                provider, model_id = row[0], str(row[1])
                names.add(model_id)
                svc_map.setdefault(model_id, provider)

    elif isinstance(raw, dict):  # very old EDSL
        for provider, lst in raw.items():
            if isinstance(lst, Sequence):
                for model_id in lst:
                    names.add(str(model_id))
                    svc_map.setdefault(str(model_id), provider)

    else:
        log.error("Unknown structure from EDSL: %r", type(raw))

    return sorted(names), svc_map


# --------------------------------------------------------------------------- #
#  Public API                                                                 #
# --------------------------------------------------------------------------- #
_SERVICE_CACHE: Dict[str, str] | None = None  # lazy singleton
_MODEL_CACHE: List[str] | None = None


def get_all_models() -> List[str]:
    """Alphabetical list of every model EDSL reports (no services)."""
    global _MODEL_CACHE, _SERVICE_CACHE

    if _MODEL_CACHE is not None:
        return _MODEL_CACHE

    if Model is None:  # pragma: no cover
        log.warning("EDSL missing – using fallback list")
        _MODEL_CACHE, _SERVICE_CACHE = ["gpt-4o"], {"gpt-4o": "openai"}
        return _MODEL_CACHE

    try:
        raw = Model.check_working_models()
        models, _SERVICE_CACHE = _normalise(raw)
        if not models:
            raise ValueError("Parsed zero models")
        _MODEL_CACHE = models
        log.info("Discovered %d unique models", len(models))
        return models
    except Exception as exc:  # pragma: no cover
        log.error("Failed to fetch models: %s", exc, exc_info=True)
        _MODEL_CACHE, _SERVICE_CACHE = ["gpt-4o"], {"gpt-4o": "openai"}
        return _MODEL_CACHE


def get_service_map() -> Dict[str, str]:
    """Return ``{model_id: service_name}``."""
    if _SERVICE_CACHE is None:
        get_all_models()  # populates the cache
    return _SERVICE_CACHE or {}


def format_models_for_selectbox() -> List[str]:
    """
    Produce strings like  ``"gpt-4o  [openai]"`` for a Streamlit selectbox.

    • Always includes **gpt-4o** and puts it on top for UX consistency.
    """
    models = get_all_models()
    svc = get_service_map()

    labelled = [f"{m} [{svc.get(m,'?')}]" for m in models]
    labelled = sorted(labelled, key=lambda s: s.lower())

    # guarantee default at top
    default = "gpt-4o [openai]"
    if default in labelled:
        labelled.remove(default)
    labelled.insert(0, default)

    log.info("Prepared %d select-box entries", len(labelled))
    return labelled


# --------------------------------------------------------------------------- #
#  Quick smoke test (only when run directly)                                  #
# --------------------------------------------------------------------------- #
if __name__ == "__main__":  # pragma: no cover
    from pprint import pprint

    pprint(format_models_for_selectbox()[:30])

# (append to the end of src/utils/models.py)


# --------------------------------------------------------------------------- #
#  Provider-lookup helper                                                     #
# --------------------------------------------------------------------------- #
def get_service_map() -> dict[str, str]:
    """
    Return a cached dict mapping **model_id ➜ provider/service name**.

    Useful for attaching the correct `service_name` when constructing `Model`.
    """
    global _SERVICE_CACHE  # type: ignore
    try:  # already built?
        return _SERVICE_CACHE
    except NameError:
        raw = Model.check_working_models()
        mapping: dict[str, str] = {}

        if isinstance(raw, list):
            for provider, model, *_ in raw:
                mapping[str(model)] = str(provider)
        elif isinstance(raw, dict):
            for provider, models in raw.items():
                for m in models:
                    mapping[str(m)] = str(provider)

        _SERVICE_CACHE = mapping
        logger.info(f"Built service map with {len(mapping)} entries")
        return mapping


# ── tests/test_agents.py

# tests/test_agents.py
import unittest
from unittest.mock import patch, MagicMock

# Mock the edsl imports
class MockAgent:
    def __init__(self, name, traits):
        self.name = name
        self.traits = traits

# Mock constants for testing
DEFAULT_PROFILES = {
    "default_a": "Default profile A",
    "default_b": "Default profile B"
}

CONVERSATION_GUIDELINES = "Test guidelines"

# Function to test
def create_agent(name, profile, default_profile):
    """Create an agent with the given name and profile."""
    return MockAgent(
        name=name,
        traits={
            "persona": profile or default_profile,
            "guidelines": CONVERSATION_GUIDELINES,
        },
    )

class TestAgents(unittest.TestCase):
    def test_create_agent_with_custom_profile(self):
        """Test creating an agent with a custom profile."""
        custom_profile = "Custom profile description"
        agent = create_agent("TestAgent", custom_profile, DEFAULT_PROFILES["default_a"])
        
        self.assertEqual(agent.name, "TestAgent")
        self.assertEqual(agent.traits["persona"], custom_profile)
        self.assertEqual(agent.traits["guidelines"], CONVERSATION_GUIDELINES)
    
    def test_create_agent_with_default_profile(self):
        """Test creating an agent with a default profile."""
        agent = create_agent("TestAgent", "", DEFAULT_PROFILES["default_a"])
        
        self.assertEqual(agent.name, "TestAgent")
        self.assertEqual(agent.traits["persona"], DEFAULT_PROFILES["default_a"])
        self.assertEqual(agent.traits["guidelines"], CONVERSATION_GUIDELINES)
        
    def test_get_rating_with_none_value(self):
        """Test that get_rating handles None values correctly."""
        # Add our implementation of get_rating function with the error handling
        import re
        
        def mock_result():
            return None
            
        # Mock the Model and other components
        class MockModel:
            def __init__(self, model_name):
                self.model_name = model_name
                
        class MockScenario:
            def __init__(self, data):
                self.data = data
                
        class MockQuestion:
            def __init__(self):
                pass
                
            def by(self, component):
                return self
                
            def run(self):
                return self
                
            def select(self, name):
                return self
                
            def first(self):
                return mock_result()
                
        # Override the rating function for testing
        def test_get_rating(model_name, agent, history_txt):
            result = mock_result()
            
            # Handle case where result might be None
            if result is None:
                return 5  # Default middle rating if no response
            
            # Try to convert to int, with fallback
            try:
                return int(result)
            except (ValueError, TypeError):
                # Try to extract a number if result is a string with text
                if isinstance(result, str):
                    numbers = re.findall(r'\d+', result)
                    if numbers:
                        return int(numbers[0])
                return 5  # Default to middle rating if conversion fails
        
        # Test with None result
        rating = test_get_rating("test-model", MockAgent("test", {}), "test history")
        self.assertEqual(rating, 5)
        
    def test_get_rating_with_string_value(self):
        """Test that get_rating handles string values correctly."""
        import re
        
        def mock_result():
            return "I rate this date a 8 out of 10"
            
        # Mock the Model and other components
        class MockModel:
            def __init__(self, model_name):
                self.model_name = model_name
                
        class MockScenario:
            def __init__(self, data):
                self.data = data
                
        class MockQuestion:
            def __init__(self):
                pass
                
            def by(self, component):
                return self
                
            def run(self):
                return self
                
            def select(self, name):
                return self
                
            def first(self):
                return mock_result()
                
        # Override the rating function for testing
        def test_get_rating(model_name, agent, history_txt):
            result = mock_result()
            
            # Handle case where result might be None
            if result is None:
                return 5  # Default middle rating if no response
            
            # Try to convert to int, with fallback
            try:
                return int(result)
            except (ValueError, TypeError):
                # Try to extract a number if result is a string with text
                if isinstance(result, str):
                    numbers = re.findall(r'\d+', result)
                    if numbers:
                        return int(numbers[0])
                return 5  # Default to middle rating if conversion fails
        
        # Test with string result containing a number
        rating = test_get_rating("test-model", MockAgent("test", {}), "test history")
        self.assertEqual(rating, 8)  # Should extract 8 from the string

if __name__ == "__main__":
    unittest.main()


# ── tests/test_models.py

"""
Tests for the model-helper utilities (live-hits EDSL).
"""

from __future__ import annotations

import pathlib
import sys
import unittest
from collections.abc import Sequence

ROOT = pathlib.Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT / "src"))

try:
    from edsl import Model  # type: ignore
except Exception:
    Model = None  # type: ignore[assignment]

from utils.models import (
    get_all_models,
    get_service_map,
    format_models_for_selectbox,
)

# Locate PrettyList for isinstance checks (EDSL moved it around a few times)
LIST_LIKE: tuple[type, ...] = (list,)
for p in ("edsl.utilities.PrettyList", "edsl.util.pretty", "edsl.pretty"):
    try:
        mod = __import__(p, fromlist=["PrettyList"])
        PrettyList = getattr(mod, "PrettyList")  # type: ignore[attr-defined]
        LIST_LIKE = (list, PrettyList)
        break
    except Exception:
        pass


def _extract(raw) -> set[str]:
    if isinstance(raw, LIST_LIKE):
        return {str(r[1]) for r in raw if isinstance(r, Sequence) and len(r) >= 2}
    if isinstance(raw, dict):
        out: set[str] = set()
        for v in raw.values():
            if isinstance(v, Sequence):
                out.update(map(str, v))
        return out
    raise TypeError(type(raw))


class TestModels(unittest.TestCase):
    @unittest.skipIf(Model is None, "EDSL not installed")
    def test_service_mapping_is_complete(self) -> None:
        """Every model reported by EDSL has an associated service."""
        edsl_models = _extract(Model.check_working_models())
        map_ = get_service_map()

        self.assertTrue(edsl_models)  # sanity
        self.assertGreaterEqual(len(map_), len(edsl_models) - 5)

        missing = [m for m in edsl_models if m not in map_]
        self.assertLess(len(missing), 5, f"Missing service for: {missing[:5]}")

        self.assertEqual(map_.get("gpt-4o"), "openai")

    def test_dropdown_strings(self) -> None:
        dd = format_models_for_selectbox()
        self.assertIn("gpt-4o [openai]", dd)
        for entry in dd[:50]:  # spot-check format
            self.assertRegex(entry, r".+\s\[[^\]]+\]")

    def test_superset_of_edsl(self) -> None:
        expected = _extract(Model.check_working_models()) if Model else set()
        actual = set(get_all_models())
        self.assertTrue(expected.issubset(actual))


if __name__ == "__main__":  # pragma: no cover
    unittest.main()


# ── tests/test_simulation.py

# tests/test_simulation.py
import unittest
from unittest.mock import patch, MagicMock

# Mock necessary components for testing
class MockAgent:
    def __init__(self, name, traits):
        self.name = name
        self.traits = traits

# Mock constants and functions
DEFAULT_PROFILES = {
    "default_a": "Default profile A",
    "default_b": "Default profile B"
}

def create_agent(name, profile, default_profile):
    return MockAgent(name, {"persona": profile or default_profile, "guidelines": "guidelines"})

def get_opener(model_name, agent):
    return "Hi there, I'm the opener!"

def get_response(model_name, agent_self, agent_other, turn, speaker, history_txt):
    return "This is a response."

def get_rating(model_name, agent, history_txt):
    return 8

# Function to test (simplified version)
def run_date(profile_a, profile_b, name_a, name_b, n_rounds, model_name, theme=None):
    # Use display names in transcript, defaulting to A/B if none provided
    display_a = name_a if name_a else "A"
    display_b = name_b if name_b else "B"
    
    # Create agents
    agent_a = create_agent(display_a, profile_a, DEFAULT_PROFILES["default_a"])
    agent_b = create_agent(display_b, profile_b, DEFAULT_PROFILES["default_b"])

    transcript = []
    
    # Add opener
    opener = get_opener(model_name, agent_a)
    transcript.append((display_a, opener))
    
    # Add one round of conversation
    for _ in range(n_rounds):
        transcript.append((display_b, get_response(model_name, agent_b, agent_a, 0, "B", "")))
        transcript.append((display_a, get_response(model_name, agent_a, agent_b, 0, "A", "")))
    
    # Get ratings
    rating_a = get_rating(model_name, agent_a, "")
    rating_b = get_rating(model_name, agent_b, "")
    
    return transcript, rating_a, rating_b

class TestSimulation(unittest.TestCase):
    def test_run_date_flow(self):
        """Test the flow of a date simulation."""
        # Run the simulation with our mock functions
        transcript, rating_a, rating_b = run_date(
            "Profile A", "Profile B", "Alice", "Bob", 1, "mock-model"
        )
        
        # Verify results
        self.assertEqual(len(transcript), 3)  # Opener + 2 responses (1 round)
        self.assertEqual(transcript[0], ("Alice", "Hi there, I'm the opener!"))
        self.assertEqual(transcript[1], ("Bob", "This is a response."))
        self.assertEqual(transcript[2], ("Alice", "This is a response."))
        self.assertEqual(rating_a, 8)
        self.assertEqual(rating_b, 8)

if __name__ == "__main__":
    unittest.main()


# ── troubleshoot.py

#!/usr/bin/env python3
"""
Troubleshooting tool for the model dropdown.

This script helps debug why the model dropdown might only be showing gpt-4o.
It checks various potential issues and provides guidance on how to fix them.
"""
import os
import sys
import logging
import importlib.util

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

print("\n==== love.dj Model Dropdown Troubleshooter ====\n")

# Check if the script is being run from the project root
print("1. Checking script location...", end=" ")
if not os.path.exists("app.py") or not os.path.exists("src"):
    print("❌ FAIL")
    print("   Error: This script should be run from the project root directory.")
    print("   Current directory:", os.getcwd())
    print("   Please run: cd /path/to/love.dj && python3 troubleshoot.py")
    sys.exit(1)
print("✅ OK")

# Check if EDSL is installed
print("2. Checking EDSL installation...", end=" ")
edsl_spec = importlib.util.find_spec("edsl")
if edsl_spec is None:
    print("❌ FAIL")
    print("   Error: EDSL is not installed.")
    print("   Please run: pip install edsl")
    print("   If you're using a virtual environment, make sure it's activated.")
    sys.exit(1)
print("✅ OK")

# Check if Model is available in EDSL
print("3. Checking EDSL Model class...", end=" ")
try:
    from edsl import Model
    print("✅ OK")
except ImportError as e:
    print("❌ FAIL")
    print(f"   Error: Could not import Model from EDSL: {e}")
    print("   Please check your EDSL installation.")
    sys.exit(1)

# Try calling check_working_models
print("4. Checking Model.check_working_models()...", end=" ")
try:
    models_by_provider = Model.check_working_models()
    print("✅ OK")
except Exception as e:
    print("❌ FAIL")
    print(f"   Error: Failed to call Model.check_working_models(): {e}")
    print("   Please check your EDSL configuration and API credentials.")
    sys.exit(1)

# Check the type of the result
print("5. Checking result type...", end=" ")
if not isinstance(models_by_provider, dict):
    print("❌ FAIL")
    print(f"   Error: Model.check_working_models() returned {type(models_by_provider)} instead of dict.")
    print(f"   Value: {models_by_provider}")
    sys.exit(1)
print("✅ OK")

# Check if there are any providers
print("6. Checking providers...", end=" ")
if not models_by_provider:
    print("❌ FAIL")
    print("   Error: No providers returned from Model.check_working_models().")
    print("   Please check your EDSL configuration and API credentials.")
    sys.exit(1)
print(f"✅ OK - Found {len(models_by_provider)} providers")

# Check if there are any models
all_models = []
for provider, models in models_by_provider.items():
    if isinstance(models, list):
        all_models.extend(models)

print("7. Checking models count...", end=" ")
if not all_models:
    print("❌ FAIL")
    print("   Error: No models found in any provider.")
    print("   Provider data:", models_by_provider)
    sys.exit(1)
print(f"✅ OK - Found {len(all_models)} models")

# Check our utility functions
print("8. Checking model utility functions...", end=" ")
try:
    from src.utils.models import get_all_models, format_models_for_selectbox
    print("✅ OK")
except ImportError as e:
    print("❌ FAIL")
    print(f"   Error: Could not import model utilities: {e}")
    print("   Please check your project structure.")
    sys.exit(1)

# Check get_all_models
print("9. Testing get_all_models()...", end=" ")
try:
    result = get_all_models()
    if len(result) <= 1:
        print("❌ FAIL")
        print(f"   Error: get_all_models() only returned {len(result)} model(s): {result}")
        print("   But Model.check_working_models() returned {len(all_models)} models.")
        print("   This suggests a bug in get_all_models().")
        sys.exit(1)
    print(f"✅ OK - Returned {len(result)} models")
except Exception as e:
    print("❌ FAIL")
    print(f"   Error: Exception in get_all_models(): {e}")
    print("   Please check the implementation of get_all_models().")
    sys.exit(1)

# Check format_models_for_selectbox
print("10. Testing format_models_for_selectbox()...", end=" ")
try:
    formatted = format_models_for_selectbox()
    if len(formatted) <= 1:
        print("❌ FAIL")
        print(f"   Error: format_models_for_selectbox() only returned {len(formatted)} model(s): {formatted}")
        print("   But get_all_models() returned {len(result)} models.")
        print("   This suggests a bug in format_models_for_selectbox().")
        sys.exit(1)
    print(f"✅ OK - Returned {len(formatted)} models")
except Exception as e:
    print("❌ FAIL")
    print(f"   Error: Exception in format_models_for_selectbox(): {e}")
    print("   Please check the implementation of format_models_for_selectbox().")
    sys.exit(1)

# All checks passed
print("\n✅ ALL CHECKS PASSED")
print(f"Found {len(models_by_provider)} providers with {len(all_models)} models.")
print(f"get_all_models() returned {len(result)} models.")
print(f"format_models_for_selectbox() returned {len(formatted)} models.")

# Show sample models
print("\nSample models from EDSL (first 10):")
for i, model in enumerate(sorted(all_models)[:10]):
    print(f"  {i+1}. {model}")

print("\nIf the Streamlit dropdown is still only showing gpt-4o:")
print("1. Make sure you're running the latest version of the code")
print("2. Check the edsl_models.log file for any errors")
print("3. Try restarting the Streamlit app")
print("4. Try clearing your browser cache or using a private browsing window")
print("5. Make sure your Streamlit app is using the updated model utils")

print("\nYou might see different models in the dropdown because:")
print("- Your EDSL configuration/API keys might only have access to certain models")
print("- The available models might change over time as providers update their offerings")
print("- Different environments might have different EDSL versions or configurations")

print("\nFor more detailed debugging, run:")
print("python3 debug_models.py > models_debug.log 2>&1")